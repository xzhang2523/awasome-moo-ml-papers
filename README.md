## awesome-moo-ml-papers

# Pareto set learning
### Multiobjective Optimization and Pareto Front Learning Resources

1. **PHN: Learning Pareto front by hypernetwork**  
   *Authors:* Navon et al  
   *Conference:* ICLR, 2019  
   *Link:* [arXiv:2010.04104](https://arxiv.org/abs/2010.04104)

2. **PHN-HVI: Improving pareto front learning via multi-sample hyper-networks**  
   *Author:* LP Hoang et al  
   *Conference:* AAAI, 2023  
   *Link:* [arXiv:2212.01130](https://arxiv.org/abs/2212.01130)

3. **PSL-Exp: Pareto set learning for expensive multiobjective optimization**  
   *Authors:* Lin et al  
   *Conference:* NeurIPS, 2022  
   *Link:* [arXiv:2210.08495](https://arxiv.org/abs/2210.08495)

4. **COSMOS: Scalable Pareto Front Approximation for Deep Multi-Objective Learning**
   *Authors:* Ruchte et al  
   *Conference:* ICDM, 2022
   *Link:* [arXiv:2103.13392.pdf](https://arxiv.org/pdf/2103.13392.pdf)

6. **PaMaL: Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models**  
   *Authors:* Dimitriadis et al  
   *Conference:* ICLR, 2023
   *Link:* [Proceedings of Machine Learning Research (PMLR)](https://proceedings.mlr.press/v202/dimitriadis23a.html)

8. **GMOOAR: Multi-objective deep learning with adaptive reference vectors**
   *Authors:* Ruchte et al  
   *Conference:* NeurIPS, 2023 
   *Link:* [NeurIPS Conference Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/d313b4a8c88eba7f0542c489899cec77-Paper-Conference.pdf)



# Pareto Multitask learning (Discrete Solutions)
1. PMTL: Pareto Multi Task Learning. NeurIPS, 2018.

2.  Nash-MTL. Navon 2022.

3.  MGDA. (Sener 2018).

4.  EPO.

5. SVGD.

6. GMOOAR. Multi-objective deep learning with adaptive reference vectors. 

7. PNG. 



# Using MOO idea to solve MTL (Single solution)

It is noticed that, this line using Pareto ideas to solve MTL. 




# Theories. 

1. HV-PSL.

2. 


# Applications in very large problems. 
A. Drug deign

B. LLM

1. Panacea: Pareto Alignment via Preference Adaptation for LLMs
https://arxiv.org/html/2402.02030v1

2. Controllable Preference Optimization.

3. 


# Awesome MOO libs

1. libMTL. Yu Zhang's group. Sustech. 

2. Libmoon. Xiaoyuan Zhang. CityU HK.
3. 
4. 

 

